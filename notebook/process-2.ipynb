{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19b5a00-d181-426d-baa8-0591c6ed4df7",
   "metadata": {},
   "source": [
    "# NOTEBOOK TO PREPROCESS THE DATA (then used for the project accomplishment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13364f-5128-4ceb-83b9-3a6fba19cb48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **INFORMATIONS ON THE CSVs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb650e4-cb35-4227-a3cd-6851911d1af3",
   "metadata": {},
   "source": [
    "*Data*: BeerAdvocate / RateBeer / matched_beer_data\n",
    "\n",
    "*Difference ratings-reviews*: **reviews.txt** appears to be a subset of **ratings.txt** because the latter also has the review column (True or False) and **reviews.txt** is the set of all ratings that are True.\n",
    "\n",
    "*Code to print .txt*: \n",
    "* \"\"\"with open(BA_REVIEWS_DATASET, 'r', encoding='utf-8') as file:\n",
    "    for _ in range(16):\n",
    "        print(file.readline())\"\"\"\n",
    "* \"\"\"with open(BA_RATINGS_DATASET, 'r', encoding='utf-8') as file:\n",
    "    for _ in range(17):\n",
    "        print(file.readline())\"\"\"\n",
    "* !head Data/BeerAdvocate/ratings.txt/ratings.txt\n",
    "* \"\"\"from collections import deque\n",
    "n_last_lines = 10\n",
    "with open(BA_REVIEWS_DATASET, 'r', encoding='utf-8') as file:\n",
    "    last_lines = deque(file, maxlen=n_last_lines)\n",
    "for line in last_lines:\n",
    "    print(line.strip())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad28347-8110-48fd-be40-eaa21f3a1e0d",
   "metadata": {},
   "source": [
    "### BeerAdvocate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994460dd-1ff9-49ef-827b-f9878e840375",
   "metadata": {},
   "source": [
    "**beers.csv**\n",
    "* beer_id\n",
    "* beer_name\n",
    "* brewery_id\n",
    "* brewery_name\n",
    "* style\n",
    "* nbr_ratings\n",
    "* nbr_reviews\n",
    "* avg\n",
    "* ba_score\n",
    "* bros_score\n",
    "* abv\n",
    "* avg_computed\n",
    "* zscore\n",
    "* nbr_matched_valid_ratings\n",
    "* avg_matched_valid_ratings\n",
    "\n",
    "**breweries.csv**\n",
    "* id,\n",
    "* location\n",
    "* name\n",
    "* nbr_beers\n",
    "\n",
    "**users.csv**\n",
    "* nbr_ratings\n",
    "* nbr_reviews\n",
    "* user_id\n",
    "* user_name\n",
    "* joined\n",
    "* location\n",
    "\n",
    "**ratings.txt** (line format i.e. Header=None)\n",
    "* beer_name\n",
    "* beer_id\n",
    "* brewery_name\n",
    "* brewery_id\n",
    "* style\n",
    "* abv\n",
    "* date\n",
    "* user_name\n",
    "* user_id\n",
    "* appearance\n",
    "* aroma\n",
    "* palate\n",
    "* taste\n",
    "* overall\n",
    "* rating\n",
    "* text\n",
    "* review: *True or False*\n",
    "\n",
    "**reviews.txt** (line format i.e. Header=None, subset of **ratings.txt**)\n",
    "* beer_name\n",
    "* beer_id\n",
    "* brewery_name\n",
    "* brewery_id\n",
    "* style\n",
    "* abv\n",
    "* date\n",
    "* user_name\n",
    "* user_id\n",
    "* appearance : *up to 5*\n",
    "* aroma : *up to 5*\n",
    "* palate : *up to 5*\n",
    "* taste : *up to 5*\n",
    "* overall : *up to 5*\n",
    "* rating : *up to 5, unkown formula but different weights for each parameter*\n",
    "* text\n",
    "\n",
    "----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e033d0c-f20c-44b8-bff0-1af5be077643",
   "metadata": {},
   "source": [
    "### RateBeer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559dc1b-7f7a-4020-808a-a2d7447455ae",
   "metadata": {},
   "source": [
    "**beers.csv**\n",
    "* beer_id\n",
    "* beer_name\n",
    "* brewery_id\n",
    "* brewery_name\n",
    "* style\n",
    "* nbr_ratings\n",
    "* overall_score\n",
    "* style_score\n",
    "* avg\n",
    "* abv\n",
    "* avg_computed\n",
    "* zscore\n",
    "* nbr_matched_valid_ratings\n",
    "* avg_matched_valid_ratings\n",
    "\n",
    "**breweries.csv**\n",
    "* id\n",
    "* location\n",
    "* name\n",
    "* nbr_beers\n",
    "\n",
    "**users.csv**\n",
    "* nbr_ratings\n",
    "* user_id\n",
    "* user_name\n",
    "* joined\n",
    "* location\n",
    "\n",
    "**ratings.txt = reviews.txt** (line format i.e. Header=None)\n",
    "* beer_name\n",
    "* beer_id\n",
    "* brewery_name\n",
    "* brewery_id\n",
    "* style\n",
    "* abv\n",
    "* date\n",
    "* user_name\n",
    "* user_id\n",
    "* appearance : *up to 5*\n",
    "* aroma : *up to 10*\n",
    "* palate (=mouthfeel) : *up to 5*\n",
    "* taste : *up to 10*\n",
    "* overall : *up to 20*\n",
    "* rating : *up to 50 (sum of all previous) then divided by 10 --> up to 5*\n",
    "* text\n",
    "\n",
    "----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d031d-d98b-4aed-9785-0cce90e4a6d7",
   "metadata": {},
   "source": [
    "### matched_beer_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8aeb9-927d-4238-867e-e59ac70f1b01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**beers.csv**\n",
    "#### ba:\n",
    "* abv\n",
    "* avg\n",
    "* avg_computed\n",
    "* avg_matched_valid_ratings\n",
    "* ba_score\n",
    "* beer_id\n",
    "* beer_name\n",
    "* beer_wout_brewery_name\n",
    "* brewery_id\n",
    "* brewery_name\n",
    "* bros_score\n",
    "* nbr_matched_valid_ratings\n",
    "* nbr_ratings\n",
    "* nbr_reviews\n",
    "* style\n",
    "* zscore\n",
    "#### rb:\n",
    "* abv\n",
    "* avg\n",
    "* avg_computed\n",
    "* avg_matched_valid_ratings\n",
    "* beer_id\n",
    "* beer_name\n",
    "* beer_wout_brewery_name\n",
    "* brewery_id\n",
    "* brewery_name\n",
    "* nbr_matched_valid_ratings\n",
    "* nbr_ratings\n",
    "* overall_score\n",
    "* style\n",
    "* style_score\n",
    "* zscore\n",
    "#### scores:\n",
    "* diff\n",
    "* sim\n",
    "\n",
    "**breweries.csv**\n",
    "#### ba:\n",
    "* id\n",
    "* location\n",
    "* name\n",
    "* nbr_beers\n",
    "#### rb:\n",
    "* id\n",
    "* location\n",
    "* name\n",
    "* nbr_beers\n",
    "#### scores:\n",
    "* diff\n",
    "* sim\n",
    "\n",
    "**ratings.csv**\n",
    "#### ba:\n",
    "* abv\n",
    "* appearance\n",
    "* aroma\n",
    "* beer_id\n",
    "* beer_name\n",
    "* brewery_id\n",
    "* brewery_name\n",
    "* date\n",
    "* overall\n",
    "* palate\n",
    "* rating\n",
    "* review\n",
    "* style\n",
    "* taste\n",
    "* text\n",
    "* user_id\n",
    "* user_name\n",
    "#### rb:\n",
    "* abv\n",
    "* appearance\n",
    "* aroma\n",
    "* beer_id\n",
    "* beer_name\n",
    "* brewery_id\n",
    "* brewery_name\n",
    "* date\n",
    "* overall\n",
    "* palate\n",
    "* rating\n",
    "* style\n",
    "* taste\n",
    "* text\n",
    "* user_id\n",
    "* user_name\n",
    "\n",
    "\n",
    "**users_approx.csv**\n",
    "#### ba:\n",
    "* joined\n",
    "* location\n",
    "* nbr_ratings\n",
    "* nbr_reviews\n",
    "* user_id\n",
    "* user_name\n",
    "* user_name_lower\n",
    "#### rb:\n",
    "* joined\n",
    "* location\n",
    "* nbr_ratings\n",
    "* user_id\n",
    "* user_name\n",
    "* user_name_lower\n",
    "#### scores:\n",
    "* sim\n",
    "\n",
    "**users.csv** (is a subset of **users_approx** --> it is composed of users from **users_approx** where `sim` closed to 1)\n",
    "#### ba:\n",
    "* joined\n",
    "* location\n",
    "* nbr_ratings\n",
    "* nbr_reviews\n",
    "* user_id\n",
    "* user_name\n",
    "* user_name_lower\n",
    "#### rb:\n",
    "* joined\n",
    "* location\n",
    "* nbr_ratings\n",
    "* user_id\n",
    "* user_name\n",
    "* user_name_lower\n",
    "\n",
    "----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c65d4-a866-4f6c-8d4d-c9cbf1f37c64",
   "metadata": {},
   "source": [
    "## **LOADING DATAs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3e5a76-1c9a-43cc-8ecd-b4f088268654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(52575) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a91fbe6-cbce-4e5a-8700-68f15e493664",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '/Users/zoemonnard/Documents/ada-2024-project-bada55/data/beer_dataset/'\n",
    "BEER_ADVOCATE_FOLDER = DATA_FOLDER + 'BeerAdvocate/' #BA\n",
    "RATE_BEER_FOLDER = DATA_FOLDER + 'RateBeer/' #RB\n",
    "\n",
    "BA_REVIEWS_DATASET = BEER_ADVOCATE_FOLDER + \"reviews.txt\"\n",
    "\n",
    "RB_REVIEWS_DATASET = RATE_BEER_FOLDER + \"reviews.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f42529-7cc8-476d-85cd-6eb2badb3498",
   "metadata": {},
   "source": [
    "## **PROCESSING BEERADVOCATE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a457d05-0d26-4191-9944-bff108cf335f",
   "metadata": {},
   "source": [
    "### ratings.csv / reviews.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7544f-45f2-420e-bb27-94f2034f691b",
   "metadata": {},
   "source": [
    "ratings.txt != reviews.txt\n",
    "\n",
    "ratings :\n",
    "[151 074 576 lines i.e. 151 074 576/18 = 8 393 032 reviews]\n",
    "\n",
    "reviews :\n",
    "[4 4022 962 lines i.e. 44 022 962/17 = 2 589 586 reviews]\n",
    "\n",
    "----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8f76f-0cb6-49f6-9b83-f574e37a7c39",
   "metadata": {},
   "source": [
    "Treatment of .txt to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54a7bc0-86da-4514-8c13-7cad36faad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../../generated/ba_chunks/ba_reviews_chunk_0.parquet\n",
      "Saved ../../generated/ba_chunks/ba_reviews_chunk_1.parquet\n",
      "Saved ../../generated/ba_chunks/ba_reviews_chunk_2.parquet\n",
      "Saving done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "columns = ['beer_name', 'beer_id', 'brewery_name', 'brewery_id', 'style', 'abv', 'date', \n",
    "           'user_name', 'user_id', 'appearance', 'aroma', 'palate', 'taste', 'overall', \n",
    "           'rating', 'text']\n",
    "chunk_size = 1_000_000\n",
    "data = []\n",
    "entry_count = 0\n",
    "chunk_count = 0\n",
    "current_entry = {}\n",
    "\n",
    "output_dir = \"../../generated/ba_chunks/\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "with open(BA_REVIEWS_DATASET, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            key, value = line.split(':', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            current_entry[key] = value\n",
    "        else:\n",
    "            if current_entry:\n",
    "                data.append(current_entry)\n",
    "                current_entry = {}\n",
    "                entry_count += 1\n",
    "\n",
    "                # Save chunk when reaching chunk size\n",
    "                if entry_count >= chunk_size:\n",
    "                    chunk_df = pd.DataFrame(data, columns=columns)\n",
    "                    chunk_file_path = f\"{output_dir}ba_reviews_chunk_{chunk_count}.parquet\"\n",
    "                    chunk_df.to_parquet(chunk_file_path)\n",
    "                    print(f\"Saved {chunk_file_path}\")\n",
    "                    data = []\n",
    "                    entry_count = 0\n",
    "                    chunk_count += 1\n",
    "                    \n",
    "# Process any remaining entries after the loop\n",
    "if data:\n",
    "    chunk_df = pd.DataFrame(data, columns=columns)\n",
    "    chunk_file_path = f\"{output_dir}ba_reviews_chunk_{chunk_count}.parquet\"\n",
    "    chunk_df.to_parquet(chunk_file_path)\n",
    "    print(f\"Saved {chunk_file_path}\")\n",
    "\n",
    "print('Saving done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fe8df7-8216-4344-b20a-8555be4ea3c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../generated/ba_chunks/ba_reviews_chunk_0.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m chunk_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumns)\n\u001b[1;32m     27\u001b[0m chunk_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../generated/ba_chunks/ba_reviews_chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m chunk_df\u001b[38;5;241m.\u001b[39mto_parquet(chunk_file_path)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ada/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ada/lib/python3.11/site-packages/pandas/core/frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   3114\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3115\u001b[0m     path,\n\u001b[1;32m   3116\u001b[0m     engine,\n\u001b[1;32m   3117\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   3118\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   3119\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[1;32m   3120\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   3121\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3122\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ada/lib/python3.11/site-packages/pandas/io/parquet.py:480\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 480\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m    481\u001b[0m     df,\n\u001b[1;32m    482\u001b[0m     path_or_buf,\n\u001b[1;32m    483\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    484\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m    485\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[1;32m    486\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    487\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    489\u001b[0m )\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ada/lib/python3.11/site-packages/pandas/io/parquet.py:349\u001b[0m, in \u001b[0;36mFastParquetImpl.write\u001b[0;34m(self, df, path, compression, index, partition_cols, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options passed with file object or non-fsspec file path\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m    350\u001b[0m         path,\n\u001b[1;32m    351\u001b[0m         df,\n\u001b[1;32m    352\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    353\u001b[0m         write_index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m    354\u001b[0m         partition_on\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    356\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ada/lib/python3.11/site-packages/fastparquet/writer.py:1343\u001b[0m, in \u001b[0;36mwrite\u001b[0;34m(filename, data, row_group_offsets, compression, file_scheme, open_with, mkdirs, has_nulls, write_index, partition_on, fixed_text, append, object_encoding, times, custom_metadata, stats)\u001b[0m\n\u001b[1;32m   1339\u001b[0m     fmd\u001b[38;5;241m.\u001b[39mkey_value_metadata \u001b[38;5;241m=\u001b[39m kvm\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_scheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;66;03m# Case 'simple'\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m     write_simple(filename, data, fmd,\n\u001b[1;32m   1344\u001b[0m                  row_group_offsets\u001b[38;5;241m=\u001b[39mrow_group_offsets,\n\u001b[1;32m   1345\u001b[0m                  compression\u001b[38;5;241m=\u001b[39mcompression, open_with\u001b[38;5;241m=\u001b[39mopen_with,\n\u001b[1;32m   1346\u001b[0m                  has_nulls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stats\u001b[38;5;241m=\u001b[39mstats)\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;66;03m# Case 'hive', 'drill'\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m     write_multi(filename, data, fmd,\n\u001b[1;32m   1350\u001b[0m                 row_group_offsets\u001b[38;5;241m=\u001b[39mrow_group_offsets,\n\u001b[1;32m   1351\u001b[0m                 compression\u001b[38;5;241m=\u001b[39mcompression, file_scheme\u001b[38;5;241m=\u001b[39mfile_scheme,\n\u001b[1;32m   1352\u001b[0m                 write_fmd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, open_with\u001b[38;5;241m=\u001b[39mopen_with,\n\u001b[1;32m   1353\u001b[0m                 mkdirs\u001b[38;5;241m=\u001b[39mmkdirs, partition_on\u001b[38;5;241m=\u001b[39mpartition_on,\n\u001b[1;32m   1354\u001b[0m                 append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stats\u001b[38;5;241m=\u001b[39mstats)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ada/lib/python3.11/site-packages/fastparquet/writer.py:1002\u001b[0m, in \u001b[0;36mwrite_simple\u001b[0;34m(fn, data, fmd, row_group_offsets, compression, open_with, has_nulls, append, stats)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     write_to_file(fn)\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1002\u001b[0m     of \u001b[38;5;241m=\u001b[39m open_with(fn, mode)\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m of \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1004\u001b[0m         write_to_file(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../generated/ba_chunks/ba_reviews_chunk_0.parquet'"
     ]
    }
   ],
   "source": [
    "columns = ['beer_name', 'beer_id', 'brewery_name', 'brewery_id', 'style', 'abv', 'date', \n",
    "           'user_name', 'user_id', 'appearance', 'aroma', 'palate', 'taste', 'overall', \n",
    "           'rating', 'text']\n",
    "chunk_size = 1_000_000\n",
    "data = []\n",
    "entry_count = 0\n",
    "chunk_count = 0\n",
    "current_entry = {}\n",
    "\n",
    "with open(BA_REVIEWS_DATASET, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "                key, value = line.split(':', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                current_entry[key] = value\n",
    "        else:\n",
    "            if current_entry:\n",
    "                data.append(current_entry)\n",
    "                current_entry = {}\n",
    "                entry_count += 1\n",
    "\n",
    "                # Save chunk when reaching chunk size\n",
    "                if entry_count >= chunk_size:\n",
    "                    chunk_df = pd.DataFrame(data, columns=columns)\n",
    "                    chunk_file_path = f\"../../generated/ba_chunks/ba_reviews_chunk_{chunk_count}.parquet\"\n",
    "                    chunk_df.to_parquet(chunk_file_path)\n",
    "                    print(f\"Saved {chunk_file_path}\")\n",
    "                    data = []\n",
    "                    entry_count = 0\n",
    "                    chunk_count += 1\n",
    "                    \n",
    "# Process any remaining entries after the loop\n",
    "if data:\n",
    "    chunk_df = pd.DataFrame(data, columns=columns)\n",
    "    chunk_file_path = f\"../../generated/ba_chunks/ba_reviews_chunk_{chunk_count}.parquet\"\n",
    "    chunk_df.to_parquet(chunk_file_path)\n",
    "    print(f\"Saved {chunk_file_path}\")\n",
    "\n",
    "print('Saving done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07090ed4-d058-44e5-83f7-1c0297446f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_chunk_files = glob.glob(\"../../generated/ba_chunks/ba_reviews_chunk_*.parquet\")\n",
    "ba_reviews = pd.concat([pd.read_parquet(ba_chunk) for ba_chunk in ba_chunk_files], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac67032-e1c5-4901-ab47-5536eccd41d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is clean, with no missing values.\n"
     ]
    }
   ],
   "source": [
    "empty_text = ba_reviews['text'] == ''\n",
    "ba_reviews.drop(ba_reviews[empty_text].index, inplace= True)\n",
    "missing_values = np.where(pd.isnull(ba_reviews))\n",
    "if missing_values[0].size == 0:\n",
    "    print(\"The dataset is clean, with no missing values.\")\n",
    "else:\n",
    "    print(\"The dataset has missing values at:\")\n",
    "    print(\"Row indices:\", missing_values[0])\n",
    "    print(\"Column indices:\", missing_values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb33848d-4250-4a9f-b2a0-7b6eef1a199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_numeric = ['beer_id', 'brewery_id', 'abv', 'date', 'appearance', 'aroma', 'palate', 'taste', 'overall', 'rating']\n",
    "ba_reviews[cols_to_numeric] = ba_reviews[cols_to_numeric].apply(pd.to_numeric, errors = 'coerce')\n",
    "ba_reviews['date'] = pd.to_datetime(ba_reviews['date'], unit='s').dt.strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb8d53a6-cefb-4421-93f5-cf8ee0aca3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute path to output files: /Users/zoemonnard/Documents/generated/ba_chunks\n"
     ]
    }
   ],
   "source": [
    "absolute_path = os.path.abspath(output_dir)\n",
    "print(\"Absolute path to output files:\", absolute_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca7c3e-4498-46fd-b930-f3db4842dbef",
   "metadata": {},
   "source": [
    "## **PROCESSING RATEBEER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549f982-d1be-4d9e-89a9-434ef25cff0c",
   "metadata": {},
   "source": [
    "### ratings.csv / reviews.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df893b-097d-4f58-a2bb-a143411147e2",
   "metadata": {},
   "source": [
    "ratings.txt = reviews.txt (i.e. no difference for this dataset)\n",
    "\n",
    "[121 075 258 lines i.e. 121075258/17 = 7 122 074 review]\n",
    "\n",
    "----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0b08c7-9a43-4801-9e7b-d486efbf1bb7",
   "metadata": {},
   "source": [
    "Treatment of .txt to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8bbfdc-38c9-4593-8e41-1a2fa92ad968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../../generated/rb_chunks/rb_reviews_chunk_0.parquet\n",
      "Saved ../../generated/rb_chunks/rb_reviews_chunk_1.parquet\n",
      "Saved ../../generated/rb_chunks/rb_reviews_chunk_2.parquet\n",
      "Saved ../../generated/rb_chunks/rb_reviews_chunk_3.parquet\n",
      "Saved ../../generated/rb_chunks/rb_reviews_chunk_4.parquet\n",
      "Saved ../../generated/rb_chunks/rb_reviews_chunk_5.parquet\n",
      "Saved ../../generated/rb_chunks/rb_reviews_chunk_6.parquet\n",
      "Saved ../../generated/rb_chunks/rb_reviews_chunk_7.parquet\n",
      "Saving done\n"
     ]
    }
   ],
   "source": [
    "columns = ['beer_name', 'beer_id', 'brewery_name', 'brewery_id', 'style', 'abv', 'date', \n",
    "           'user_name', 'user_id', 'appearance', 'aroma', 'palate', 'taste', 'overall', \n",
    "           'rating', 'text']\n",
    "chunk_size = 1_000_000\n",
    "data = []\n",
    "entry_count = 0\n",
    "chunk_count = 0\n",
    "current_entry = {}\n",
    "\n",
    "with open(RB_REVIEWS_DATASET, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "                key, value = line.split(':', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                current_entry[key] = value\n",
    "        else:\n",
    "            if current_entry:\n",
    "                data.append(current_entry)\n",
    "                current_entry = {}\n",
    "                entry_count += 1\n",
    "\n",
    "                # Save chunk when reaching chunk size\n",
    "                if entry_count >= chunk_size:\n",
    "                    chunk_df = pd.DataFrame(data, columns=columns)\n",
    "                    chunk_file_path = f\"../../generated/rb_chunks/rb_reviews_chunk_{chunk_count}.parquet\"\n",
    "                    chunk_df.to_parquet(chunk_file_path)\n",
    "                    print(f\"Saved {chunk_file_path}\")\n",
    "                    data = []\n",
    "                    entry_count = 0\n",
    "                    chunk_count += 1\n",
    "                    \n",
    "# Process any remaining entries after the loop\n",
    "if data:\n",
    "    chunk_df = pd.DataFrame(data, columns=columns)\n",
    "    chunk_file_path = f\"../../generated/rb_chunks/rb_reviews_chunk_{chunk_count}.parquet\"\n",
    "    chunk_df.to_parquet(chunk_file_path)\n",
    "    print(f\"Saved {chunk_file_path}\")\n",
    "\n",
    "print('Saving done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a931d9-978f-4207-9ad5-6bb343e6737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_chunk_files = glob.glob(\"../../generated/rb_chunks/rb_reviews_chunk_*.parquet\")\n",
    "rb_reviews = pd.concat([pd.read_parquet(rb_chunk) for rb_chunk in rb_chunk_files], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "443901c7-5e8a-400d-a11d-0c7014c23f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is clean, with no missing values.\n"
     ]
    }
   ],
   "source": [
    "empty_text = rb_reviews['text'] == ''\n",
    "rb_reviews.drop(rb_reviews[empty_text].index, inplace= True)\n",
    "np.where(pd.isnull(rb_reviews))\n",
    "if missing_values[0].size == 0:\n",
    "    print(\"The dataset is clean, with no missing values.\")\n",
    "else:\n",
    "    print(\"The dataset has missing values at:\")\n",
    "    print(\"Row indices:\", missing_values[0])\n",
    "    print(\"Column indices:\", missing_values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62c7ed77-4da1-42fb-8820-a6e3a01fd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_numeric = ['beer_id', 'brewery_id', 'abv', 'date', 'appearance', 'aroma', 'palate', 'taste', 'overall', 'rating']\n",
    "rb_reviews[cols_to_numeric] = rb_reviews[cols_to_numeric].apply(pd.to_numeric, errors = 'coerce')\n",
    "rb_reviews['date'] = pd.to_datetime(rb_reviews['date'], unit='s').dt.strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077c931-bfd2-4e80-bce2-395487dd380a",
   "metadata": {},
   "source": [
    "## **SAVING PROCESSED DFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295599da-55a2-4c1c-a61e-65462fad02ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_reviews.to_parquet('../../generated/new_ba_reviews.parquet')\n",
    "rb_reviews.to_parquet('../../generated/new_rb_reviews.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
