





























import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import glob


DATA_FOLDER = '../../data/'
BEER_ADVOCATE_FOLDER = DATA_FOLDER + 'BeerAdvocate/' #BA
RATE_BEER_FOLDER = DATA_FOLDER + 'RateBeer/' #RB

BA_RATINGS_DATASET = BEER_ADVOCATE_FOLDER + 'ratings.txt/' + "ratings.txt"
BA_REVIEWS_DATASET = BEER_ADVOCATE_FOLDER + 'reviews.txt/' + "reviews.txt"

RB_RATINGS_DATASET = RATE_BEER_FOLDER + 'ratings.txt/' + "ratings.txt"
RB_REVIEWS_DATASET = RATE_BEER_FOLDER + 'reviews.txt/' + "reviews.txt"

BA_USERS_DATASET = BEER_ADVOCATE_FOLDER + "users.csv"
RB_USERS_DATASET = RATE_BEER_FOLDER + "users.csv"


ba_users = pd.read_csv(BA_USERS_DATASET)
rb_users = pd.read_csv(RB_USERS_DATASET)








ba_users_joined = ba_users.copy()
ba_users_joined = ba_users_joined.drop(['nbr_ratings', 'nbr_reviews', 'user_name', 'location'], axis= 1)
ba_users_joined = ba_users_joined.dropna()
# remove duplicates
ba_users_joined = ba_users_joined.drop_duplicates(subset='user_id', keep='first')  # keep the first occurrence of each duplicate
ba_users_joined['joined'] = pd.to_datetime(ba_users_joined['joined'], unit='s')
ba_users_joined['user_id'] = ba_users_joined['user_id'].astype(str)











columns = ['beer_name', 'beer_id', 'brewery_name', 'brewery_id', 'style', 'abv', 'date', 
           'user_name', 'user_id', 'appearance', 'aroma', 'palate', 'taste', 'overall', 
           'rating', 'text']
chunk_size = 1_000_000
data = []
entry_count = 0
chunk_count = 0
current_entry = {}

with open(BA_REVIEWS_DATASET, 'r', encoding='utf-8') as file:
    for line in file:
        line = line.strip()
        if line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                current_entry[key] = value
        else:
            if current_entry:
                data.append(current_entry)
                current_entry = {}
                entry_count += 1

                # Save chunk when reaching chunk size
                if entry_count >= chunk_size:
                    chunk_df = pd.DataFrame(data, columns=columns)
                    chunk_file_path = f"../../generated/ba_chunks/ba_reviews_chunk_{chunk_count}.parquet"
                    chunk_df.to_parquet(chunk_file_path)
                    print(f"Saved {chunk_file_path}")
                    data = []
                    entry_count = 0
                    chunk_count += 1
                    
# Process any remaining entries after the loop
if data:
    chunk_df = pd.DataFrame(data, columns=columns)
    chunk_file_path = f"../../generated/ba_chunks/ba_reviews_chunk_{chunk_count}.parquet"
    chunk_df.to_parquet(chunk_file_path)
    print(f"Saved {chunk_file_path}")

print('Saving done')


ba_chunk_files = glob.glob("../../generated/ba_chunks/ba_reviews_chunk_*.parquet")
ba_reviews = pd.concat([pd.read_parquet(ba_chunk) for ba_chunk in ba_chunk_files], ignore_index=True)


empty_text = ba_reviews['text'] == ''
ba_reviews.drop(ba_reviews[empty_text].index, inplace= True)
missing_values = np.where(pd.isnull(ba_reviews))
if missing_values[0].size == 0:
    print("The dataset is clean, with no missing values.")
else:
    print("The dataset has missing values at:")
    print("Row indices:", missing_values[0])
    print("Column indices:", missing_values[1])





print(f"Shape before keeping ratings from users in users_joined: {ba_reviews.shape}")
ba_reviews = ba_reviews[ba_reviews['user_id'].isin(ba_users_joined['user_id'])]
print(f"Shape after keeping ratings from users in users_joined: {ba_reviews.shape}")


print(set(ba_reviews['user_id']).issubset(set(ba_users_joined['user_id'])))


cols_to_numeric = ['beer_id', 'brewery_id', 'abv', 'date', 'appearance', 'aroma', 'palate', 'taste', 'overall', 'rating']
ba_reviews[cols_to_numeric] = ba_reviews[cols_to_numeric].apply(pd.to_numeric, errors = 'coerce')
ba_reviews['date'] = pd.to_datetime(ba_reviews['date'], unit='s')








rb_users_joined = rb_users.copy()
rb_users_joined = rb_users_joined.drop(['nbr_ratings', 'user_name', 'location'], axis= 1)
rb_users_joined = rb_users_joined.dropna()
# remove duplicates
rb_users_joined = rb_users_joined.drop_duplicates(subset='user_id', keep='first')  # keep the first occurrence of each duplicate
rb_users_joined['joined'] = pd.to_datetime(rb_users_joined['joined'], unit='s')
rb_users_joined['user_id'] = rb_users_joined['user_id'].astype(str)











columns = ['beer_name', 'beer_id', 'brewery_name', 'brewery_id', 'style', 'abv', 'date', 
           'user_name', 'user_id', 'appearance', 'aroma', 'palate', 'taste', 'overall', 
           'rating', 'text']
chunk_size = 1_000_000
data = []
entry_count = 0
chunk_count = 0
current_entry = {}

with open(RB_REVIEWS_DATASET, 'r', encoding='utf-8') as file:
    for line in file:
        line = line.strip()
        if line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                current_entry[key] = value
        else:
            if current_entry:
                data.append(current_entry)
                current_entry = {}
                entry_count += 1

                # Save chunk when reaching chunk size
                if entry_count >= chunk_size:
                    chunk_df = pd.DataFrame(data, columns=columns)
                    chunk_file_path = f"../../generated/rb_chunks/rb_reviews_chunk_{chunk_count}.parquet"
                    chunk_df.to_parquet(chunk_file_path)
                    print(f"Saved {chunk_file_path}")
                    data = []
                    entry_count = 0
                    chunk_count += 1
                    
# Process any remaining entries after the loop
if data:
    chunk_df = pd.DataFrame(data, columns=columns)
    chunk_file_path = f"../../generated/rb_chunks/rb_reviews_chunk_{chunk_count}.parquet"
    chunk_df.to_parquet(chunk_file_path)
    print(f"Saved {chunk_file_path}")

print('Saving done')


rb_chunk_files = glob.glob("../../generated/rb_chunks/rb_reviews_chunk_*.parquet")
rb_reviews = pd.concat([pd.read_parquet(rb_chunk) for rb_chunk in rb_chunk_files], ignore_index=True)


empty_text = rb_reviews['text'] == ''
rb_reviews.drop(rb_reviews[empty_text].index, inplace= True)
np.where(pd.isnull(rb_reviews))
if missing_values[0].size == 0:
    print("The dataset is clean, with no missing values.")
else:
    print("The dataset has missing values at:")
    print("Row indices:", missing_values[0])
    print("Column indices:", missing_values[1])





print(f"Shape before keeping ratings from users in users_joined: {rb_reviews.shape}")
rb_reviews = rb_reviews[rb_reviews['user_id'].isin(rb_users_joined['user_id'])]
print(f"Shape after keeping ratings from users in users_joined: {rb_reviews.shape}")


print(set(rb_reviews['user_id']).issubset(set(rb_users_joined['user_id'])))


cols_to_numeric = ['beer_id', 'brewery_id', 'abv', 'date', 'appearance', 'aroma', 'palate', 'taste', 'overall', 'rating']
rb_reviews[cols_to_numeric] = rb_reviews[cols_to_numeric].apply(pd.to_numeric, errors = 'coerce')
rb_reviews['date'] = pd.to_datetime(rb_reviews['date'], unit='s')





ba_reviews.to_parquet('../../generated/new_ba_reviews.parquet')
rb_reviews.to_parquet('../../generated/new_rb_reviews.parquet')


ba_users_joined.to_parquet('../../generated/ba_users_joined.parquet')
rb_users_joined.to_parquet('../../generated/rb_users_joined.parquet')
